pip install gensim
import numpy as np
import pandas as pd
import nltk
import gensim

from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

#使用nltk包
nltk.download('punkt')  
nltk.download('stopwords')

# Load data into dataframe           
df = pd.read_csv('watch_reviews.tsv', sep='\t', error_bad_lines=False) # error_bad....: by default 15行，这里有22行，多的扔掉了

# Remove missing value
df.dropna(subset=['review_body'],inplace=True)
df.info()

# use the first 1000 data as our training data
data = df.loc[:1000, 'review_body'].tolist() 

#Tokenizing and Stemming
# Use nltk's English stopwords.
stopwords = nltk.corpus.stopwords.words('english')  #stopwords包
stopwords.append("'s")
stopwords.append("'m")
stopwords.append("n't")
stopwords.append("br")
print ("We use " + str(len(stopwords)) + " stop-words from nltk library.")
print (stopwords[:10])

from nltk.stem.snowball import SnowballStemmer
# REGULAR EXPRESSION
import re

stemmer = SnowballStemmer("english")

# tokenization and stemming
def tokenization_and_stemming(text):
    tokens = []
    # exclude stop words and tokenize the document, generate a list of string 
    for word in nltk.word_tokenize(text):
        if word.lower() not in stopwords:
            tokens.append(word.lower())

    filtered_tokens = []
    
    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
    for token in tokens:
        if re.search('[a-zA-Z]', token): #re:正则表达式， filter只要文字的，不用数字和特殊符号
            filtered_tokens.append(token)
            
    # stemming：整合不同时态的词， 但取词根很暴力
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems
tokenization_and_stemming(data[0])
    
#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
# define vectorizer parameters   n_grams一般到七个就差不多了
# TfidfVectorizer will help us to create tf-idf matrix
# max_df : maximum document frequency for the given word
# min_df : minimum document frequency for the given word
# max_features: maximum number of words
# use_idf: if not true, we only calculate tf
# stop_words : built-in stop words
# tokenizer: how to tokenize the document
# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram
tfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000, 
                                 min_df=0.01, stop_words='english',
                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))

tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses

print ("In total, there are " + str(tfidf_matrix.shape[0]) + \
      " reviews and " + str(tfidf_matrix.shape[1]) + " terms.")
      
# check the parameters
tfidf_model.get_params()
# words
tf_selected_words = tfidf_model.get_feature_names()
# print out words 241
tf_selected_words

from sklearn.cluster import KMeans
num_clusters = 5 #boss asks
# number of clusters
km = KMeans(n_clusters=num_clusters)
km.fit(tfidf_matrix)
clusters = km.labels_.tolist()

#Analyze K-means Result
# create DataFrame films from all of the input files.
product = { 'review': df[:1000].review_body, 'cluster': clusters}
frame = pd.DataFrame(product, columns = ['review', 'cluster'])

print ("Number of reviews included in each cluster:")
frame['cluster'].value_counts().to_frame()

print ("<Document clustering result by K-means>")

#km.cluster_centers_ denotes the importances of each items in centroid.
#We need to sort it in decreasing-order and get the top k items.
order_centroids = km.cluster_centers_.argsort()[:, ::-1] 

Cluster_keywords_summary = {}
for i in range(num_clusters):
    print ("Cluster " + str(i) + " words:", end='')
    Cluster_keywords_summary[i] = []
    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster
        Cluster_keywords_summary[i].append(tf_selected_words[ind])
        print (tf_selected_words[ind] + ",", end='')
    print ()
    
    cluster_reviews = frame[frame.cluster==i].review.tolist()
    print ("Cluster " + str(i) + " reviews (" + str(len(cluster_reviews)) + " reviews): ")
    print (", ".join(cluster_reviews))
    print ()
    
#opic Modeling - Latent Dirichlet Allocation
# Use LDA for clustering
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=5)

from sklearn.feature_extraction.text import CountVectorizer
# LDA requires integer values  LDA输入必须是integer，不能有小数
tfidf_model_lda = CountVectorizer(max_df=0.99, max_features=500,
                                 min_df=0.01, stop_words='english',
                                 tokenizer=tokenization_and_stemming, ngram_range=(1,1))

tfidf_matrix_lda = tfidf_model_lda.fit_transform(data) #fit the vectorizer to synopses

print ("In total, there are " + str(tfidf_matrix_lda.shape[0]) + \
      " reviews and " + str(tfidf_matrix_lda.shape[1]) + " terms.")
# document topic matrix for tfidf_matrix_lda   1000 documents， 5 topics的probability加起来等于1
lda_output = lda.fit_transform(tfidf_matrix_lda)
print(lda_output.shape)
print(lda_output)

# topics and words matrix   每一个feature（词）在这个topic里的重要性
topic_word = lda.components_
print(topic_word.shape)
print(topic_word) 

# column names
topic_names = ["Topic" + str(i) for i in range(lda.n_components)]

# index names
doc_names = ["Doc" + str(i) for i in range(len(data))]

df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)

# get dominant topic for each document
topic = np.argmax(df_document_topic.values, axis=1)
df_document_topic['topic'] = topic
df_document_topic.head(10)
df_document_topic['topic'].value_counts().to_frame()

# topic word matrix
print(lda.components_)
# topic-word matrix
df_topic_words = pd.DataFrame(lda.components_)

# column and index
df_topic_words.columns = tfidf_model_lda.get_feature_names()
df_topic_words.index = topic_names
df_topic_words.head()

# print top n keywords for each topic
def print_topic_words(tfidf_model, lda_model, n_words):
    words = np.array(tfidf_model.get_feature_names())
    topic_words = []
    # for each topic, we have words weight
    for topic_words_weights in lda_model.components_:
        top_words = topic_words_weights.argsort()[::-1][:n_words]
        topic_words.append(words.take(top_words))
    return topic_words

topic_keywords = print_topic_words(tfidf_model=tfidf_model_lda, lda_model=lda, n_words=15)        

df_topic_words = pd.DataFrame(topic_keywords)
df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]
df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]
df_topic_words
    
